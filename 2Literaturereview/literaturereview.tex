\chapter{LITERATURE REVIEW}
%1.............................................................................
\section{Literature Review I}
The paper titled Voice Emotion Recognition in Conversations Using Artificial Intelligence, A Systematic Review and Meta Analysis by Leontios Hadjileontiadis was a comprehensive and well-organized analysis of the development of research in the field of conversational voice emotion recognition\cite{lr1}. This review was conducted in contrast to traditional studies that were based on isolated utterances, whereas natural human conversation involved emotional expression that depended on context, interaction patterns, and speaker relationships\cite{lr1}. The research emphasized that emotion recognition in real conversations was more complex than emotion detection from single sentences because emotions evolved over time, reacted to other speakers, and were highly situation-dependent\cite{lr1}.
A large body of research literature was systematically structured and evaluated through a rigorous review methodology\cite{lr1}. The review provided a broad overview of artificial intelligence methods used in conversational emotion recognition, including classical feature-based approaches, temporal modeling using recurrent neural networks, spectral representations with convolutional neural networks, long-range dependency modeling through attention mechanisms, and contextual sequence learning using transformer-based architectures. The meta-analysis evaluated performance improvements across these approaches and demonstrated that deep learning models significantly outperformed traditional methods\cite{lr1}.
The development of multimodal conversational datasets was also highlighted in the review\cite{lr1}. These datasets combined audio, textual, visual gestures, and physiological signals, enabling more accurate emotion recognition\cite{lr1}. Multimodal systems achieved superior performance because conversational emotions were often influenced by subtle contextual cues that could not be captured through voice alone. However, several challenges were identified, including the scarcity of high-quality conversational datasets, inconsistencies in emotion annotation schemes, cross-cultural variability in emotional expression, and difficulties in modeling overlapping emotions during conversations\cite{lr1}.
The review critically discussed the limitations of existing systems, noting that models trained on acted datasets with exaggerated emotions performed poorly in real conversational environments characterized by background noise, informal speech, interruptions, sarcasm, and uncertainty. Context modeling, speaker diarization, and sequence-based learning were identified as essential components for improving system robustness. Future research directions included cross-cultural adaptation, context-aware emotion tracking, ethical considerations in emotion monitoring, unified benchmarking standards, and real-time adaptive systems, with applications in virtual assistants, call centers, and mental health monitoring contributing to more natural humanâ€“computer interaction\cite{lr1}.

\section{Literature Review II}
The research paper entitled Emotion Recognition on Call Center Voice Data explored the significance of voice emotion recognition in improving customer service activities in call centers\cite{lr2}. The analysis was based on real customer interactions rather than controlled or acted speech, making the study more practical in real-world applications. Call center conversations were often characterized by emotional strain, dissatisfaction, indecisiveness, and urgency, which made emotion detection an essential tool for understanding customer needs\cite{lr2}. As demonstrated in the study, identifying customer emotions during interactions enabled agents to respond more empathetically and effectively. Unlike earlier emotion recognition studies that relied on clean datasets, this research addressed challenges associated with noisy and spontaneous speech\cite{lr2}. Authentic call recordings from Turkish mobile operators were used, incorporating background noise, varied speaking styles, and interrupted speech, thereby closely simulating real call center environments and increasing the difficulty of emotion recognition. This realistic analysis allowed the study to present a practical evaluation of system performance and usability\cite{lr2}.
The system employed deep learning techniques to classify emotional states into positive, negative, and neutral categories\cite{lr2}. This simplified classification scheme was suitable for operational environments where fast and direct emotional feedback was required. An accuracy of 0.91 was reported, demonstrating that deep learning models could successfully learn emotional patterns even from complex and unstructured voice data\cite{lr2}. The system provided emotional intelligence support to call center employees, helping them adapt their communication strategies during customer interactions. The authors concluded that integrating emotion recognition into call center workflows improved customer satisfaction and service quality\cite{lr2}. The study also emphasized the importance of language-specific research, as the system was developed for Turkish speech, a language that is underrepresented in emotion recognition studies. The findings suggested that future systems could be extended to multilingual environments and enhanced with finer-grained emotion categories for more detailed emotional analysis\cite{lr2}.
Another significant contribution of the research was the investigation of feature selection and preprocessing techniques to optimize emotion classification under real-world conditions\cite{lr2}. Noise filtering, voice segmentation, and spectral feature extraction methods were applied to improve input data quality. The study also identified speaker variability as a factor influencing model performance and discussed approaches to reduce bias across different voice types and accents\cite{lr2}. These techniques supported the development of more robust and generalizable emotion recognition systems that could be effectively deployed in real call center application settings.
\section{Literature Review III}
The paper titled Emotion Detection via Voice and Voice Recognition, written by Rohit Rastogi, Tushar Anand, Shubham Sharma, and Sarthak Panwar, was a thorough investigation of detecting human emotions through the spoken voice cues\cite{lr3}. The paper has started by highlighting the increasingly critical role of emotion-conscious computing in the context whereby intelligent systems were interacting with human beings more frequently. Voice being natural and impulsive had substantial acoustic signals that disclosed the emotional condition of a person even when the verbal message itself was neutral\cite{lr3}. The objective of the study was to develop a strong model that would explain these emotional cues through the combination of state-of-the-art signal processing and machine learning methods.
The study has described the audio preprocessing steps to be taken in recognizing emotions in detail\cite{lr3}. Voice cues that were observed across different people contained huge differences in tone, volume, noise in the background, and the manner of speaking. Direct processing of audio content in its pure form resulted in the model paying attention to unrelated anomalies and not emotional content\cite{lr3}. In order to solve this, the pipeline was utilized that incorporated noise removal, silence clipping, feature scaling, and transformation of audio signals into homogeneous segments of length. The result of this process was pure and reliable representations of voice that had a direct effect on the accuracy of classifiers\cite{lr3}.
The paper has reinforced the role of feature extraction\cite{lr3}. There was a lot of use of Mel Frequency Cepstral Coefficients and other spectral and temporal descriptors. These features measured vital characteristics of emotions including change in energy, change in pitch dynamics, harmonic structure, and frequency distribution\cite{lr3}. Emotions like anger were linked with more energy, more harmonics, and broader frequencies, whereas sadness was reflected in less energetic, less harmonic, and less high-pitched voice. The emotion recognition model was based on these differences in acoustics\cite{lr3}.
Classical machine learning algorithms as well as deep learning architectures were tried\cite{lr3}. Conventional algorithms such as Support Vector Machines and Random Forest classifiers performed well using well-designed features. Nevertheless, deep learning models, especially convolutional neural networks, had a much higher performance as compared to the classical models\cite{lr3}. The space patterns that were found to be related to emotional tone in convolutional layers were automatically identified and the use of manual feature engineering was minimized.
The findings showed that certain emotions were more classifiable as compared to others\cite{lr3}. Emotions with extreme acoustic profiles such as anger and happiness were better recorded, whereas similar acoustic emotions such as calm and sad caused confusion\cite{lr3}. The analysis indicated the need of bigger more varied datasets in order to enhance generalization. Human-computer interaction, personalized digital assistants, driver monitoring systems, and emotion-aware recommendation engines were also used in practice\cite{lr3}. The work laid a solid foundation to contemporary voice-based affective computing.
\section{Literature Review IV}
The paper Evaluated the Impact of Voice Activity Detection on Voice Emotion Recognition in Autistic Children aimed at enhancing the rate of emotion recognition during child voice especially with autism\cite{lr4}. The voice of autistic children also did not correspond to usual patterns of speech and contained uneven timing, abnormal intonation, and prolonged pauses. These attributes complicated emotion recognition and decreased the efficiency of regular emotion recognition systems\cite{lr4}. The authors focused on preprocessing methods, especially voice activity detection, to extract useful voice segments. Silence, background noise, and unwanted audio contents were eliminated with voice activity detection and then emotion analysis occurred\cite{lr4}. Such a step was necessary for child voice, in which emotional indications were subtle and easily concealed by noise or non-vowel sounds.
The study showed through experimental assessment that the reliability of emotional feature extraction with voice activity detection had been enhanced\cite{lr4}. More stable results of emotion recognition were achieved through better segmentation of voice. The study also revealed that preprocessing was vital in the overall performance of voice emotion recognition systems, particularly in dealing with sensitive and varying voice data\cite{lr4}. The paper emphasized the need for special emotion recognition systems responsive to vulnerable groups like autistic children. Ethical issues regarding data gathering, system implementation, and interpretation of emotional cues were also highlighted\cite{lr4}.
Another contribution made by the study was demonstrating the effect of adaptive preprocessing on system robustness\cite{lr4}. The results indicated that voice activity detection, when carefully tuned, improved the extraction of subtle emotional features even with difficult recordings. The paper also provided guidance on designing child-centered emotion recognition systems and suggested that more customized approaches would enhance reliability and usability in therapeutic, educational, and clinical contexts\cite{lr4}. These findings highlighted the significance of preprocessing in the creation of useful, specialized emotion-aware technologies that can be applied to sensitive groups.

\section{Literature Review V}
The system introduced by Sadil Chamishka, Ishara Madhavi, Rashmika Nawaratne, Damminda Alahakoon, Daswin De Silva, Naveen Chilamkurti, and Vishaka Nanayakkara was a voice-based emotion recognition system which worked in real time to detect temporal variations in emotion in voice using recurrent neural networks and refined feature modelling\cite{lr5}. The procedure was based on the creation of RNN constructions that preserved the sequential dependence of acoustic characteristics, enabling the system to acquire dynamic variation of vocal expressions much more effectively than inherent classifiers. The architecture operated over voice streams and had been scaled to low-latency real-time applications, which was highly useful in interactive systems\cite{lr5}.To increase emotional discrimination, feature modelling was applied to better represent prosodic and spectral voice features that were inputted into the RNN classifier\cite{lr5}. This allowed detection of transitioning emotional states with greater precision by combining temporal modelling with advanced feature extraction because of continuous voice input. The technique was tested on benchmark emotional voice datasets, and the experimental findings revealed significant improvement in recognition accuracy compared to machine learning models trained in baseline conditions\cite{lr5}. The RNN-based structure was more accurate than traditional classifiers and demonstrated the advantages of temporal dependency modelling. The system was qualitatively stable in processing continuous voice streams and tracking changes in emotions in real time, which was necessary for practical human-computer interaction use\cite{lr5}.
Some of the strengths identified by the study included the design of a real-time architecture, effective temporal modelling through recurrent neural networks, and enhanced feature representation resulting in superior recognition of emotions\cite{lr5}. The low-latency nature of the framework made it highly practical, and benchmark validation empirically supported the research methodology. Limitations included limited multilingual coverage, susceptibility to real-world noise, lack of multimodal inputs such as facial expressions or physiological signals, and absence of robustness analysis in adverse acoustic settings, which constrained large-scale use\cite{lr5}.Another contribution of this study was demonstrating the benefits of ageing temporal modelling with stronger feature representation in continuous emotion recognition\cite{lr5}. The research provided guidance on designing real-time interactive voice emotion systems and highlighted future directions, such as covering more languages, integrating multimodal signals, and testing robustness in unfavorable acoustic environments. The results facilitated the development of robust, precise, and practically implementable emotion-sensitive systems\cite{lr5}.


\section{Literature Review VI}
The study titled A Cross Cultural Investigation of Emotion Inferences of Voice and Voice: Implications of Voice Technology was conducted by Klaus R. Scherer, in which the perception of vocal emotions across different cultural backgrounds had been systematically examined\cite{lr6}. The expression of emotions through voice had been considered a complex interaction of physiological mechanisms, learned cultural conventions, linguistic habits, and individual expressive styles. Through this investigation, an attempt had been made to determine whether emotional cues conveyed through voice had been universally interpreted or whether they had been significantly influenced by cultural conditioning \cite{lr6}.
Voice samples representing a range of emotional states had been collected and presented to listeners belonging to multiple cultural groups\cite{lr6}. Participants had been instructed to identify emotions based solely on acoustic cues such as pitch variation, intensity, speech rate, rhythm, and intonation patterns, without access to contextual or semantic information\cite{lr6}. This approach had ensured that emotional inference was derived purely from vocal characteristics rather than external factors.
The results had indicated that certain emotional patterns had been consistently recognized across cultures. High pitch, rapid tempo, and increased intensity had been commonly associated with emotions such as fear and excitement, while low pitch, reduced energy, and slower tempo had been largely linked with sadness\cite{lr6}. These findings had supported the hypothesis that some vocal expressions of emotion had biological foundations shared across human populations\cite{lr6}.
The notable cultural variations had also been identified\cite{lr6}. Emotional interpretations had differed depending on social norms, linguistic traditions, and cultural exposure\cite{lr6}. For instance, louder vocal expressions had been perceived as anger in some cultures, whereas the same acoustic patterns had been interpreted as confidence or enthusiasm in others\cite{lr6}. These discrepancies had demonstrated that emotional perception through voice had not been entirely universal\cite{lr6}.
The implications for voice-based technologies had been strongly emphasized in the study\cite{lr6}. It had been suggested that emotion recognition systems trained on culturally limited datasets were likely to perform poorly when deployed across diverse populations\cite{lr6}. The research had highlighted the necessity of incorporating culturally diverse training data and adaptive mechanisms capable of adjusting emotional interpretations based on user background\cite{lr6}.
Another significant contribution of the study had been the emphasis on sociocultural awareness in emotion recognition system design\cite{lr6}. The findings had shown that accurate emotional interpretation could not rely solely on acoustic features without consideration of cultural context. By integrating cultural sensitivity into algorithmic models, both recognition accuracy and user acceptance had been improved. This work had provided foundational insights for the development of globally applicable and socially aware voice emotion recognition systems\cite{lr6}.

\section{Literature Review VII}
The paper titled Multimodal Integration of Emotional Signals of Voice, Body and Context had examined how emotional understanding had been influenced when voice, body movement, and contextual information had been jointly considered in communication systems\cite{lr7}. It had been argued that emotional expression had rarely been conveyed through voice alone and had instead been formed through an integrated combination of vocal tone, physical gestures, and situational cues\cite{lr7}. The study had been positioned within the domain of human-robot interaction, where emotional intelligence had been considered essential for social acceptance and effective interaction\cite{lr7}. Emotional perception accuracy had been evaluated under conditions where emotional cues had been either congruent or incongruent across modalities\cite{lr7}. It had been observed that recognition performance had been significantly improved when voice, body language, and contextual signals had conveyed consistent emotional information, whereas mismatched cues had caused ambiguity and reduced trust in robotic systems\cite{lr7}. User attitudes toward robots had been analyzed, and robots displaying multimodal emotional consistency had been perceived as more intelligent, reliable, and socially engaging\cite{lr7}. The findings had indicated that emotionally coherent multimodal behavior had positively influenced user comfort, interaction quality, and long-term acceptance\cite{lr7}. Practical deployment considerations had also been addressed, where multimodal emotion systems had been shown to perform more robustly under dynamic and unpredictable real-world conditions\cite{lr7}. Additionally, it had been suggested that future systems should incorporate extended sensory channels and long-term user interaction studies to enhance emotional adaptability and realism\cite{lr7}. Overall, the study had reinforced that multimodal emotion recognition had better reflected natural human communication patterns and had been critical for developing socially intelligent artificial agents\cite{lr7}.


\section{Literature Review VIII}
The study titled Voice Based Emotion Recognition with Convolutional Neural Networks in Companion Robots had focused on integrating voice-based emotion recognition capabilities into companion robotic systems designed for close human interaction\cite{lr8}. Companion robots had been widely applied in healthcare assistance, elderly support, child interaction, and personal companionship, where emotional awareness had been essential for natural engagement\cite{lr8}. The research objective had been centered on constructing a convolutional neural network architecture capable of classifying emotional states using voice signals\cite{lr8}. Voice recordings had been transformed into spectrogram representations, which had been treated as two-dimensional inputs enabling spatial and temporal feature learning\cite{lr8}. Emotional characteristics such as high energy and spectral sharpness for anger and reduced frequency variation for sadness had been implicitly learned by convolutional filters without manual feature extraction\cite{lr8}. Data preprocessing techniques such as normalization, segmentation, and augmentation had been applied to improve generalization across speakers and recording environments\cite{lr8}. The CNN model had been composed of stacked convolutional and pooling layers followed by fully connected layers responsible for emotion classification\cite{lr8}. Training had been performed using backpropagation on labeled emotional voice samples, and improved robustness against noise had been demonstrated when compared to traditional machine learning approaches\cite{lr8}. Real-time deployment within robotic platforms had enabled adaptive behavioral responses such as tone adjustment and interaction modulation based on detected emotions\cite{lr8}. It had also been emphasized that performance had been further enhanced when diverse accents, background noise, and spontaneous speech patterns had been included during training\cite{lr8}. The study had concluded that combining voice emotion recognition with additional modalities could further strengthen emotional intelligence in companion robots operating in complex social environments\cite{lr8}.

\section{Literature Review IX}
The paper by Fatemeh Noroozi, Tomasz Sapinski, Dorota Kaminska, and Gholamreza Anbarjafari introduced a vocal-based emotion recognition framework in which ensemble learning was implemented to model paralinguistic voice features and classify emotions accurately\cite{lr9}. The basic algorithm used prosodic and spectral data, including pitch, intensity, formants, and other paralinguistic representations as input to a random forest classifier composed of multiple decision trees. This ensemble architecture was designed to provide greater robustness and generalization by aggregating predictions from multiple weak learners rather than relying on a single classifier\cite{lr9}.The system was tested on the SAVEE emotional voice database using two validation strategies: leave-one-out cross-validation and 9-fold cross-validation\cite{lr9}. Quantitative results reported an average score of 76.28 across six emotion categories. Happiness achieved the highest recognition rate of 78, indicating that positive affective states were more easily recognized. The effectiveness of the ensemble learning approach was demonstrated through its superior performance over Linear Discriminant Analysis and deep neural network baselines on the same dataset\cite{lr9}. Qualitative analysis revealed that the system remained stable in classifying emotional categories and that strength improved when multiple decision trees were aggregated. The framework was therefore considered generalizable to multi-class emotion categorization and suitable for emotion-sensitive human-computer interaction applications\cite{lr9}.Strengths of the study included a well-structured methodological process based on ensemble learning that enhanced classification stability and minimized overfitting\cite{lr9}. Emotional separability improved through the use of discriminative paralinguistic features, and reliability increased with cross-validation strategies. Empirical evidence of the approach's superiority was presented by comparison with LDA and DNN baselines. The model demonstrated practical applicability in emotion-sensitive HCI systems\cite{lr9}.Limitations were also observed\cite{lr9}. Experiments were conducted on a single small benchmark dataset, limiting generalizability. Reliance on acted emotional voices reduced ecological validity. Multimodal emotion cues such as facial expressions or physiological signals were not integrated, restricting the scope of emotion representation. Certain emotional categories, such as fear and surprise, had low recognition accuracy, and real-time deployment was not tested, leaving system latency and practicality unexamined\cite{lr9}.Another point raised by the research was the potential of combining this ensemble learning structure with dynamic adaptive systems\cite{lr9}. The emotion recognition system could be enhanced by incorporating real-time user responses and lifelong learning to better handle different speaker traits, spontaneous facial expressions, and contextual variations. This adaptation could improve reliability, practicality, and applicability in various real-life human-computer interaction scenarios\cite{lr9}.


\section{Literature Review X}
The article titled Factor Analysis Based Speaker Normalisation for Continuous Emotion Prediction by Ting Dang, Vidhyasaharan Sethu, and Eliathamby Ambikairajah focused on the important issue of variability in speaker voices in emotion recognition systems\cite{lr10}. The authors stated that voice generated by different people was inherently different because of physiological variations in vocal tract length, habitual speaking styles, accent, gender, age, and emotional expressiveness. These differences disrupted the model's ability to focus solely on emotional cues\cite{lr10}. When a system was trained on data from a small group of speakers, it tended to overfit to particular speaker characteristics instead of learning emotion-related features. Speaker variability was identified as a key reason why continuous emotion prediction remained unstable in cross-speaker conditions\cite{lr10}.The study project a speaker normalization method based on factor analysis\cite{lr10}. The technique decomposed voice elements mathematically into components representing the speaker and components conveying emotion. This separation allowed the model to focus on variations in arousal, valence, and dominance, the three continuous dimensions typically used to describe emotional states. Unlike categorical emotion classification, continuous emotion prediction monitored the strength of emotions over time, which was claimed to be more suitable for real-world applications such as dialog systems, behavioral analytics, and affective monitoring\cite{lr10}.Voice features were decomposed into latent variables using factor analysis\cite{lr10}. This enabled speaker-specific characteristics to be distinguished and their influence minimized. Emotional cues became more evident and consistent across different speakers. Large-scale experiments on benchmark datasets with diverse speakers and emotional levels demonstrated that speaker normalization significantly improved the smoothness and accuracy of predicted emotional trajectories\cite{lr10}.A notable strength of the study was the discussion of how subtle shifts in pitch or energy due to speaker identity could be confused with emotional changes\cite{lr10}. Such variations were more problematic for continuous emotion models compared to categorical models, making normalization particularly important. Unnormalized models produced jagged, unpredictable, and noisy emotional predictions, whereas normalized models generated more stable and interpretable emotional curves\cite{lr10}.The study also highlighted that speaker normalization frameworks could be integrated with deep learning architectures to further improve cross-speaker adaptability\cite{lr10}. Neural network pipelines that included normalization allowed emotional representations to be less dependent on individual speaker characteristics, leading to more effective deployment in multiethnic populations. This design emphasized that preprocessing and normalization were as crucial as model architecture for achieving high-quality continuous emotion recognition\cite{lr10}.

\clearpage 
\section{Summary of Literature Review}

\vspace{8mm}

\renewcommand{\arraystretch}{1.3}
\setlength{\LTpre}{0pt}
\setlength{\LTpost}{0pt}

\begin{longtable}{|p{2.3cm}|p{2.2cm}|p{3.1cm}|p{3.1cm}|p{2.9cm}|}
\hline
\textbf{Author/Year} & \textbf{Focus} & \textbf{Methodology} & \textbf{Key Findings} & \textbf{Contributions} \\
\hline
\endfirsthead

\hline
\textbf{Author/Year} & \textbf{Focus} & \textbf{Methodology} & \textbf{Key Findings} & \textbf{Contributions} \\
\hline
\endhead



Hadjileontiadis (2025) &
Voice emotion recognition in conversations &
Systematic review of CNN, RNN, Transformer, and attention models &
Deep learning significantly outperforms traditional approaches; multimodal data enhances accuracy &
Comprehensive roadmap for conversational emotion recognition research \\ 
\hline

Yurtay et al. (2024) &
Emotion recognition on call center data &
Deep learning on spontaneous noisy call recordings &
Achieves 91\% accuracy despite real-world noise and variability &
Demonstrates feasibility in industrial call center environments \\ 
\hline

Rastogi et al. (2023) &
Emotion detection from voice signals &
MFCC extraction with ML and CNN classifiers &
CNN outperforms traditional ML techniques &
Effective pipeline for HCI and smart monitoring systems \\ 
\hline

Chamishka et al. (2022) &
Real-time voice emotion detection &
RNN with advanced feature modeling &
High accuracy in real-time emotion detection &
Supports deployment of emotion-aware applications \\ 
\hline

Milling et al. (2022) &
Emotion recognition for autistic children &
Voice activity detection with optimized preprocessing &
Improved classification of subtle vocal cues &
Highlights preprocessing importance for child-centered systems \\ 
\hline

Scherer (2021) &
Cross-cultural emotion inference &
Comparative acoustic analysis across cultures &
Universal emotional patterns with cultural variations identified &
Supports culturally adaptive recognition systems \\ 
\hline

Tsiourti et al. (2019) &
Multimodal emotion recognition for robots &
Integration of voice, gestures, and context &
Multimodal congruent cues improve recognition &
Guidelines for socially intelligent human-robot interaction \\ 
\hline

Alu et al. (2017) &
Emotion recognition for companion robots &
CNN on spectrogram representations &
CNN shows robustness and real-time applicability &
Enables adaptive emotional responses in robots \\ 
\hline

Noroozi et al. (2017) &
Vocal-based emotion recognition &
Random Forest and Decision Tree classifiers &
Tree models effective with low computational cost &
Suitable for lightweight real-time systems \\ 
\hline

Dang et al. (2016) &
Speaker normalization in emotion prediction &
Factor analysis-based speaker normalization &
Reduces speaker variability and improves prediction smoothness &
Enhances cross-speaker continuous emotion recognition \\ 
\hline

\caption{Summary of Literature Review}
\label{tab:literature_summary}
\end{longtable}