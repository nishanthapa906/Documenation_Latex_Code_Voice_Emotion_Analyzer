%-------------------- PAPER 10 --------------------
\section{Literature Review of Base Paper- IX}
\onehalfspacing

\begin{table}[H]
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{p{8.69cm}p{0.07cm}p{7.98cm}p{0.07cm}}
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Author(s)/Source: } Fatemeh Noroozi, Tomasz Sapiński, Dorota Kamińska, Gholamreza Anbarjafari }} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Title: } Vocal-Based Emotion Recognition Using Random Forests and Decision Tree }} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Website: } https://link.springer.com/article/10.1007/s10772-017-9396-2 }} \\ 
\hline
\multicolumn{2}{|p{8.76cm}}{{ \textbf{Publication Date: } 2017 }} & 
\multicolumn{2}{|p{8.05cm}|}{{ \textbf{Access Date: } November, 2021 }} \\ 
\hline
\multicolumn{2}{|p{8.76cm}}{{ \textbf{Publisher/Journal: } International Journal of voice Technology }} & 
\multicolumn{2}{|p{8.05cm}|}{{ \textbf{Place: } Springer, New York }} \\ 
\hline
\multicolumn{2}{|p{8.76cm}}{{ \textbf{Volume: } 20 }} & 
\multicolumn{2}{|p{8.05cm}|}{{ \textbf{Issue Number: } 2 }} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Author's position/theoretical position: } The authors propose a vocal-based emotion recognition framework that leverages random forest ensembles and decision tree classifiers to model paralinguistic voice features for accurate emotion classification. }} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Keywords: } Vocal Emotion Recognition, Random Forests, Decision Tree, Paralinguistic Features, voice Processing, Human–Computer Interaction }} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{\underline{Important points, notes, quotations } \hfill \underline{Page No.}} }
\begin{enumerate}
    \vspace{-0.2cm} \item Proposes a vocal-based emotion recognition system using random forests. \hfill \textbf{1}
    \vspace{-0.5cm} \item Uses prosodic and paralinguistic voice features for classification. \hfill \textbf{2}
    \vspace{-0.5cm} \item Evaluated on the SAVEE emotional voice database. \hfill \textbf{4}
    \vspace{-0.5cm} \item Applies leave-one-out cross-validation and 10-fold validation. \hfill \textbf{6}
    \vspace{-0.5cm} \item Achieves an average recognition rate of 66.28\%. \hfill \textbf{8}
    \vspace{-0.5cm} \item Best recognition rate of 78\% for happiness emotion. \hfill \textbf{9}
    \vspace{-0.5cm} \item Outperforms LDA and DNN baselines on the same dataset. \hfill \textbf{10}
    \vspace{-0.5cm} \item Suitable for human–computer interaction applications. \hfill \textbf{12}
\end{enumerate}
} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Essential Background Information: } Vocal emotion recognition relies on extracting paralinguistic features such as pitch, intensity, formants, and spectral properties from voice signals. Ensemble learning methods like random forests improve robustness by aggregating multiple decision trees to handle feature variability and classification uncertainty. }} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Overall argument or hypothesis: } Combining random forest ensembles with carefully selected paralinguistic voice features can significantly enhance the accuracy and robustness of vocal-based emotion recognition systems. }} \\ 
\hline
\multicolumn{4}{|p{16.82cm}|}{{ \textbf{Conclusion: } The  random forest–based emotion recognition framework effectively classifies six emotional states from voice signals, outperforming conventional LDA and DNN approaches and demonstrating strong potential for real-world human–computer interaction systems. }} \\ 
\hline
\multicolumn{1}{|p{9.69cm}}{{ \textbf{Supporting Reasons}}} & 
\multicolumn{2}{p{8.05cm}|}{} \\ 
\hhline{~~~}
\multicolumn{1}{|p{8.69cm}}{{ 1. Uses ensemble learning for robust classification. }} & 
\multicolumn{2}{p{8.05cm}|}{{ 2. Extracts discriminative paralinguistic features. }} \\ 
\hhline{~~~}
\multicolumn{1}{|p{8.69cm}}{{ 3. Validated on SAVEE dataset. }} & 
\multicolumn{2}{p{8.05cm}|}{{ 4. Employs cross-validation for reliability. }} \\ 
\hhline{~~~}
\multicolumn{1}{|p{8.69cm}}{{ 5. Demonstrates higher accuracy than LDA. }} & 
\multicolumn{2}{p{8.05cm}|}{{ 6. Outperforms deep neural networks. }} \\ 
\hhline{~~~}
\multicolumn{1}{|p{8.69cm}}{{ 7. Handles multi-class emotion classification. }} & 
\multicolumn{2}{p{8.05cm}|}{{ 8. Applicable to HCI environments. }} \\ 
\hline
\multicolumn{3}{|p{16.74cm}|}{{ \textbf{Strengths of the line of reasoning and supporting evidence: } Strong methodological design, ensemble-based classification, empirical evaluation on a benchmark dataset, comparative analysis with prior methods, and relevance to practical emotion-aware systems. }} \\ 
\hline
\multicolumn{3}{|p{16.74cm}|}{{ \textbf{Flaws in the argument and gaps or other weaknesses: } Limited dataset size, reliance on acted emotions, absence of multimodal fusion, moderate performance for fear and surprise classes, and lack of real-time deployment evaluation. }} \\ 
\hline
\end{tabular}
\end{adjustbox}
\end{table}
