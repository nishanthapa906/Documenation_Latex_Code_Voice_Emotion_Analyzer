\chapter{DISCUSSION AND ANALYSIS}

\begin{enumerate}

\item \textbf{Comparison of Theoretical and Simulated Outputs:} \\
The theoretical design of the Voice Emotion Analyzer assumed clean, noise-free audio signals with consistent speaking styles. Under these assumptions, stable emotion classification accuracy was expected. During simulated and real-world evaluation, performance variations were observed due to environmental noise, microphone quality, and speaker-dependent characteristics. The classification accuracy was computed as:
\begin{equation}
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{2020}{2408} = 0.8389
\end{equation}
which corresponded to 83.89\% correct classification. These discrepancies occurred because real-time audio acquisition introduced distortions that affected preprocessing and feature stability. The macro-averaged performance metrics were calculated to evaluate class-balanced performance:
\begin{equation}
\text{Precision}_{\text{macro}} = \frac{1}{7}\sum_{i=1}^{7} \text{Precision}_i = 0.8369
\end{equation}
\begin{equation}
\text{Recall}_{\text{macro}} = \frac{1}{7}\sum_{i=1}^{7} \text{Recall}_i = 0.8326
\end{equation}
\begin{equation}
\text{F1-Score}_{\text{macro}} = \frac{1}{7}\sum_{i=1}^{7} \text{F1-Score}_i = 0.8339
\end{equation}
These metrics demonstrated consistent performance across all seven emotion classes.

\item \textbf{Influence of Preprocessing on Model Behavior:} \\
Theoretically, normalization and noise reduction were assumed to fully eliminate recording inconsistencies. In practice, preprocessing reduced but did not completely remove noise and silence artifacts, especially in live audio recordings. The feature normalization was performed using z-score standardization:
\begin{equation}
x_{\text{normalized}} = \frac{x - \mu}{\sigma}
\end{equation}
where $x$ represented the raw feature vector, $\mu$ denoted the mean, and $\sigma$ represented the standard deviation across the training set. Minor variations in $\mu$ and $\sigma$ during live recording resulted in slight deviations in extracted features. The weighted average metrics were computed to account for class imbalance:
\begin{equation}
\text{Precision}_{\text{weighted}} = \frac{\sum_{i=1}^{7} n_i \cdot \text{Precision}_i}{\sum_{i=1}^{7} n_i} = 0.8393
\end{equation}
\begin{equation}
\text{Recall}_{\text{weighted}} = \frac{\sum_{i=1}^{7} n_i \cdot \text{Recall}_i}{\sum_{i=1}^{7} n_i} = 0.8389
\end{equation}
\begin{equation}
\text{F1-Score}_{\text{weighted}} = \frac{\sum_{i=1}^{7} n_i \cdot \text{F1-Score}_i}{\sum_{i=1}^{7} n_i} = 0.8382
\end{equation}
where $n_i$ represented the number of samples support for emotion class $i$. The close alignment between weighted and macro averages validated the preprocessing effectiveness.

\item \textbf{Error Analysis and Sources of Misclassification:} \\
Most classification errors were observed between acoustically similar emotions. Analysis of the confusion patterns revealed that sadness, happiness, and neutral emotions exhibited overlapping acoustic characteristics. The performance metrics for these emotion classes were:
\begin{equation}
\text{Sad: Precision} = 0.8012, \quad \text{Recall} = 0.8110, \quad \text{F1-Score} = 0.8061
\end{equation}
\begin{equation}
\text{Happy: Precision} = 0.8202, \quad \text{Recall} = 0.7927, \quad \text{F1-Score} = 0.8062
\end{equation}
\begin{equation}
\text{Neutral: Precision} = 0.8551, \quad \text{Recall} = 0.9282, \quad \text{F1-Score} = 0.8901
\end{equation}
The nearly identical F1-scores for sadness 0.8061 and happiness 0.8062 confirmed the acoustic similarity that led to misclassification. These errors arose due to overlapping spectral and prosodic features. Since emotional intensity exists on a continuum, discrete categorical labeling introduced inherent ambiguity at class boundaries. Notably, the neutral class achieved the highest recall 92.82\%, indicating that the model successfully identified low-arousal emotional states with greater confidence.

\item \textbf{Effect of Dataset Constraints on Generalization:} \\
The training dataset primarily contained acted emotional speech recorded in controlled studio environments. While theoretical assumptions considered this data representative of real-world emotions, practical testing revealed performance degradation on spontaneous speech samples. The performance comparison between emotions with equal support revealed:
\begin{equation}
\text{Fear 329 samples: Precision} = 0.8615, \quad \text{Recall} = 0.8511, \quad \text{F1-Score} = 0.8563
\end{equation}
\begin{equation}
\text{Disgust 329 samples: Precision} = 0.8227, \quad \text{Recall} = 0.7477, \quad \text{F1-Score} = 0.7834
\end{equation}
Despite having identical support sizes, disgust exhibited significantly lower recall 74.77\% compared to fear 85.11\%. The F1-score difference was quantified as:
\begin{equation}
\Delta_{\text{F1}} = \text{F1}_{\text{fear}} - \text{F1}_{\text{disgust}} = 0.8563 - 0.7834 = 0.0729
\end{equation}
This gap highlighted that dataset size alone did not guarantee performance; the inherent acoustic subtlety and variability of disgust expressions made them harder to classify. Limited demographic diversity and language variability further constrained cross-speaker generalization, particularly for speakers with subtle emotional expressions.

\item \textbf{Comparison with State-of-the-Art Emotion Recognition Systems:} \\
Compared to traditional machine learning classifiers such as Support Vector Machines and Random Forests, the convolutional neural network architecture employed in this work demonstrated superior feature learning capabilities and improved robustness. The emotion-specific performance metrics were:
\begin{equation}
\text{Angry: Precision} = 0.8963, \quad \text{Recall} = 0.8272, \quad \text{F1-Score} = 0.8604
\end{equation}
\begin{equation}
\text{Surprise: Precision} = 0.8014, \quad \text{Recall} = 0.8706, \quad \text{F1-Score} = 0.8346
\end{equation}
The anger class achieved the highest precision 89.63\%, while surprise demonstrated the highest recall 87.06\%. Although attention-based transformer models and hybrid LSTM-CNN architectures reported in recent literature achieved marginally higher accuracies 85-92\%, they required substantially larger datasets more than 50,000 samples and higher computational resources. The precision-recall trade-off for each emotion class was measured as:
\begin{equation}
\Delta_{\text{PR}} = |\text{Precision} - \text{Recall}| < 0.07 \quad \text{for all classes}
\end{equation}
This indicated a well-balanced classifier with minimal bias toward false positives or false negatives, making it suitable for real-time deployment.

\item \textbf{Methodological Performance Evaluation:} \\
The adopted methodology demonstrated superior performance in real-time deployment scenarios due to efficient MFCC-based feature extraction and a lightweight CNN architecture. Data augmentation techniques pitch shifting, time stretching, noise injection and regularization methods dropout, batch normalization were employed to enhance robustness and prevent overfitting. The system performance metrics were:
\begin{equation}
\text{Accuracy} = 0.8389, \quad \text{Total Test Samples} = 2408
\end{equation}
The variation in class-wise F1-scores was analyzed to assess performance consistency:
\begin{equation}
\text{F1-Score}_{\text{min}} = 0.7834 \text{ Disgust}, \quad \text{F1-Score}_{\text{max}} = 0.8901 \text{ Neutral}
\end{equation}
\begin{equation}
\text{F1-Score Range} = \text{F1}_{\text{max}} - \text{F1}_{\text{min}} = 0.1067
\end{equation}
The range of 0.1067 indicated moderate performance variation across emotion classes. The model performed comparably to complex architectures in clean audio conditions but showed reduced accuracy in highly noisy environments, revealing a trade-off between computational efficiency and peak performance. The alignment between macro and weighted averages was measured as:
\begin{equation}
\text{Precision Deviation} = |\text{Precision}_{\text{macro}} - \text{Precision}_{\text{weighted}}| = 0.0024
\end{equation}
\begin{equation}
\text{Recall Deviation} = |\text{Recall}_{\text{macro}} - \text{Recall}_{\text{weighted}}| = 0.0063
\end{equation}
These minimal deviations confirmed that the observed performance was not significantly affected by class imbalance, validating the robustness of the evaluation.

\end{enumerate}

\section{Classification Performance Summary}

Table~\ref{tab:classification-metrics} presents the comprehensive classification metrics obtained from the test dataset. The precision, recall, and F1-score values for each emotion class, along with macro and weighted averages, are provided to facilitate quantitative analysis of the system's performance across different emotional categories.

\begin{table}[H]
\centering
\caption{Classification Metrics for Voice Emotion Analyzer}
\label{tab:classification-metrics}
\vspace{0.3cm}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Emotion} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\ \hline
Angry & 0.8963 & 0.8272 & 0.8604 & 324 \\ \hline
Disgust & 0.8227 & 0.7477 & 0.7834 & 329 \\ \hline
Fear & 0.8615 & 0.8511 & 0.8563 & 329 \\ \hline
Happy & 0.8202 & 0.7927 & 0.8062 & 328 \\ \hline
Neutral & 0.8551 & 0.9282 & 0.8901 & 515 \\ \hline
Sad & 0.8012 & 0.8110 & 0.8061 & 328 \\ \hline
Surprise & 0.8014 & 0.8706 & 0.8346 & 255 \\ \hline
\hline
\textbf{Accuracy} & \multicolumn{4}{c|}{\textbf{0.8389}} \\ \hline
\textbf{Macro Average} & 0.8369 & 0.8326 & 0.8339 & 2408 \\ \hline
\textbf{Weighted Average} & 0.8393 & 0.8389 & 0.8382 & 2408 \\ \hline
\end{tabular}
\end{table}

The analysis revealed that while the Voice Emotion Analyzer achieved satisfactory accuracy of 83.89\%, significant variation existed in emotion-specific performance. The model demonstrated particular strength in identifying neutral emotions F1-Score = 0.8901 and exhibited consistent performance across anger, fear, and surprise categories. 