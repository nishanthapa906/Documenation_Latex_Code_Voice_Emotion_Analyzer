\chapter*{\centerline{ABSTRACT}}
\addcontentsline{toc}{chapter}{ABSTRACT}
\thispagestyle{plain}
\vspace{-0.5cm}
Voice recognition as a feature of human emotions is an influential feature of affective computing and intelligent human-computer interaction. This Project presents an automatic emotion classifier based on convolutional neural network using speech cues which attempts to establish how one can correctly predict emotion states using varying acoustic environments. The framework isolates the important feature of the acoustics such as the frequency characteristics, energy profiles as well as the pitch dynamics to create discriminative feature representation to use in classification. The model is trained to identify seven basic emotional conditions, including happiness, sadness, anger, fear, disgust, neutral, and surprise, and at the same time, pitch based gender recognition is also performed to better understand the situation. Strict experimental validation on benchmark emotion speech datasets reveals the mean classification accuracy of 83.89\% and it indicates that the model is capable of high performance in the case of diverse speakers and emotional expressions. The stability and the external validity of the suggested method are statistically proven. This work pushes the current state of the art in voice-based emotion recognition and allows practical use in mental health monitoring, where automated emotional testing aids in early intervention and continuous patient care; emotionally adaptive human-computer interfaces that provide a better user experience by increasing engagement and satisfaction in response to the emotional state of the user; and emotionally adaptive human-computer interfaces that optimize user engagement and user satisfaction by ensuring continuous interaction with the user.
\par
\textbf{Keywords: Convolutional Neural Networks,  Emotion Recognition, Frequency Patterns Pitch Analysis}