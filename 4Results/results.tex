\chapter{RESULTS}

\section{Signup Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/signuppage.png}
    \caption{SignUp Page}
    \label{fig:signup_page}
\end{figure}
The Signup Page had been designed to allow new users to create an account by entering the required personal and login details.  
It ensured secure registration and validation before granting access to the system.

\section{Login Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/Login_page.png}
    \caption{Login Page}
    \label{fig:login_page}
\end{figure}
The Login Page had been implemented to authenticate registered users using their valid credentials.  
It verified user identity and provided secure access to system functionalities after successful login.


\section{Landing Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/final_homepage.png}
    \caption{Landing Page}
    \label{fig:landing_page}
\end{figure}

This screen allows users to access the platform. The layout is designed for easy navigation, with options to log in, sign up, or explore the system features.

\section{Activity Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/home_page_final.png}
    \caption{Activity Page}
    \label{fig:activity_page}
\end{figure}

The activity page displays user interactions with the system, including recorded audio history and previously analyzed emotion results. It provides an overview of ongoing user sessions.

\section{Live Audio Prediction}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/live_audio_final.png}
    \caption{Live Audio Prediction}
    \label{fig:live_audio_prediction}
\end{figure}

This interface allows real-time emotion detection from live audio input. Users can speak into the microphone, and the system predicts the corresponding emotional state instantly.

\section{Emotion Distribution for Live Inputs}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/Emotion_distribution_of_live.png}
    \caption{Emotion Distribution for Live Inputs}
    \label{fig:emotion_distribution_live}
\end{figure}

The figure shows the distribution of predicted emotions from live recordings. It highlights which emotions are most frequently detected and validates the diversity of the model predictions.

\section{Live Audio Waveforms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/disgust_waves1.png}
    \caption{Live Audio Waveforms}
    \label{fig:live_wave_forms}
\end{figure}

Waveforms of live audio inputs illustrate the signal intensity and time variation of different emotional tones. These visualizations aid in understanding the characteristics captured by the model.

\section{Live Audio Spectrograms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/disgust_spectrogram.png}
    \caption{Live Audio Spectrograms}
    \label{fig:live_spectrogram}
\end{figure}

Spectrograms display frequency content over time for live audio. They provide a detailed insight into tonal and spectral features used by the CNN for emotion recognition.

\section{Live Audio Suggestions}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/Suggestion_live.png}
    \caption{Live Audio Suggestions}
    \label{fig:live_Suggestions}
\end{figure}


\section{File Upload Prediction}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/File_upload_final.png}
    \caption{File Upload Prediction}
    \label{fig:file_upload_prediction}
\end{figure}

This screen demonstrates emotion prediction for uploaded audio files. Users can select pre-recorded audio, and the system predicts the associated emotion accurately.

\section{File Upload Waveforms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/file_uplod_wave_form.png}
    \caption{File Upload Waveforms}
    \label{fig:file_upload_wave_forms}
\end{figure}

Waveforms of uploaded files show audio amplitude variations, providing a visual representation of emotional expression in the recordings.

\section{File Upload Spectrograms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/fileupload_spectorgram.png}
    \caption{File Upload Spectrograms}
    \label{fig:file_upload_spectrogram}
\end{figure}

Spectrograms for uploaded files visualize frequency distribution over time, helping analyze the characteristics of audio signals and their emotional content.

\section{Emotion Distribution for Uploaded Files}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/emotion_distributio_of_upload.png}
    \caption{Emotion Distribution for Uploaded Files}
    \label{fig:emotion_distribution_upload}
\end{figure}

This chart represents the distribution of predicted emotions from uploaded audio. It validates the consistency and accuracy of predictions compared to live input results.


\section{File Upload Suggestions}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/suggestion_file_upload.png}
    \caption{File Upload Suggestions}
    \label{fig:file_upload}
\end{figure}



\section{History Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/History_final.png}
    \caption{History Page}
    \label{fig:history}
\end{figure}

The history page records previous user interactions, including analyzed audio files and their predicted emotions. This feature provides traceability and insights into user behavior over time.




\clearpage







\section{Training and Validation Loss}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/Graphics/Accuracyonlycurve.png}
    \caption{Training and Validation Accuracy Curve}
    \label{fig:enter-label}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/Graphics/lossonlycurve.png}
    \caption{Training and Validation Loss Curve}
    \label{fig:enter-label}
\end{figure}
\clearpage
The training and validation loss curves represented the learning process of the model during the training process. The observed high loss values were initially as a result of random creation of the parameters but these were quickly reduced as the convolutional neural network acquired acoustic properties like the pitch, energy, and spectral properties. Training loss became small in more recent epochs, which means that it is well-optimized and that learning is stable. The validation loss was almost in the same direction but with slightly bigger values as compared to training loss and did not exhibit any sudden spikes, indicating that there was no overfitting. In line with this, validation accuracy had increased gradually with low initial values before reaching high values in final epochs just like the training accuracy. Such a small difference between the training and validation measures meant that the model was able to generalize to unseen data and not just memorise training samples. All in all, the steadily declining loss curves and augmenting trends in accuracy all attested to the fact that the proposed CNN-based voice emotion recognition model was trained appropriately, managed to attain healthy generalization, and show great performance on test data without overfitting.




\section{Confusion Matrix}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/Graphics/final_confusionmatrix.png}
    \caption{Confusion Matrix}
    \label{fig:enter-label}
\end{figure}
The confusion matrix was used to analyze the model of voice emotion recognition accuracy on the test data in terms of the predicted labels and the actual emotion label. Right predictions were placed in the diagonal whereas wrongful classifications were placed off-diagonal. The model performed well in all the categories of emotion, but angry emotions were correctly recognized as such because of acquired acoustic properties like extreme intensity and acute changes of pitch. The emotions of disgust and fear were recognized with the highest rates and the lowest level of confusion, but the slight differences with happy and neutral categories were observed as the tones are similar. Happy emotion was consistently categorized even with slight confusion with fear and disgust due to slight overlapping patterns of pitch and energy. The most classification proved to be accurate with neutral emotion with most samples being identified correctly and a slight confusion with sad or happy emotion. The classification of sad emotion was good, but a few of them were confused with the neutral emotion, as they had the same low-energy and slow-tempo vocals. Surprise emotion was well categorized with minimal misclassifications as disgust or happy, and this illustrates that the model has the ability to identify sudden high-energy vocal patterns. In general, the confusion matrix was highly dominated by diagonals, meaning that the process of classification has been very effective in all the emotion classes and the error has been primarily between analogous emotions in acoustical processes thus confirming the usefulness of the convolutional neural network structure in recognizing emotions by voice.

\section{Best Case Scenario: Voice Emotion Analyzer}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/Graphics/BestCasereal.png}
    \caption{Best Case}
    \label{fig:best-label}
\end{figure}
The most optimistic was reflected by the model making accurate predictions with high confidence of neutral emotion when a male voice was used as an input. The most discriminatory acoustic features were those of neutral emotion with constant pitch contours, low levels of variation in energy, and no or minimal spectral variation. The male voice had distinct and well-articulated features that were well-acquired through the MFCC feature extraction process. The training of the model in a variety of neutral samples allowed it to acquire strong representations of serene emotional conditions. The audio signal was properly normalized by the preprocessing pipeline, which eliminated the background noise against the audio signal and improved the relevant features. Such a combination of specific emotional features, pronounced voice patterns and the ability to extract the features led to the best classification performance that justified the ability of the model to detect neutral emotional moods in real-life situations.

\section{Worst Case Scenario: Voice Emotion Analyzer}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/Graphics/worstcase1.png}
    \caption{Worst Case}
    \label{fig:worst-label}
\end{figure}
The most awful was when the model tried to make predictions of emotions of a female voice sample; prediction confidence and misclassifications were low. The fundamental frequencies and harmonic structure of female voices usually are higher and they have diverse harmonic patterns than those of male voices, thus providing acoustic variability that is not well reflected in the training data. The deterioration in the performance of the model was explained by gender-specific peculiarities of the voice that did not follow the predominantly male-trained patterns. Also, the feature extraction process could not capture subtle emotional expressions of female speech, which were described by the low intensity variations and the fine pitch variations. The demographic differences in the training data were limited, and this limited the generalizability of the model to the other speaker gender. This pointed out a very significant weakness that needs gendered data and dynamic preprocessing methods to enhance the accuracy of cross-gender emotion recognition.
\clearpage
