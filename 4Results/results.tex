\chapter{RESULTS}

\section{Signup Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/signuppage.png}
    \caption{SignUp Page}
    \label{fig:signup_page}
\end{figure}
The Signup Page had been designed to allow new users to create an account by entering the required personal and login details.  
It ensured secure registration and validation before granting access to the system.

\section{Login Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/Login_page.png}
    \caption{Login Page}
    \label{fig:login_page}
\end{figure}
The Login Page had been implemented to authenticate registered users using their valid credentials.  
It verified user identity and provided secure access to system functionalities after successful login.


\section{Landing Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/final_homepage.png}
    \caption{Landing Page}
    \label{fig:landing_page}
\end{figure}

This screen allows users to access the platform. The layout is designed for easy navigation, with options to log in, sign up, or explore the system features.

\section{Activity Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/home_page_final.png}
    \caption{Activity Page}
    \label{fig:activity_page}
\end{figure}

The activity page displays user interactions with the system, including recorded audio history and previously analyzed emotion results. It provides an overview of ongoing user sessions.

\section{Live Audio Prediction}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/live_audio_final.png}
    \caption{Live Audio Prediction}
    \label{fig:live_audio_prediction}
\end{figure}

This interface allows real-time emotion detection from live audio input. Users can speak into the microphone, and the system predicts the corresponding emotional state instantly.

\section{Emotion Distribution for Live Inputs}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/Emotion_distribution_of_live.png}
    \caption{Emotion Distribution for Live Inputs}
    \label{fig:emotion_distribution_live}
\end{figure}

The figure shows the distribution of predicted emotions from live recordings. It highlights which emotions are most frequently detected and validates the diversity of the model predictions.

\section{Live Audio Waveforms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/disgust_waves1.png}
    \caption{Live Audio Waveforms}
    \label{fig:live_wave_forms}
\end{figure}

Waveforms of live audio inputs illustrate the signal intensity and time variation of different emotional tones. These visualizations aid in understanding the characteristics captured by the model.

\section{Live Audio Spectrograms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/disgust_spectrogram.png}
    \caption{Live Audio Spectrograms}
    \label{fig:live_spectrogram}
\end{figure}

Spectrograms display frequency content over time for live audio. They provide a detailed insight into tonal and spectral features used by the CNN for emotion recognition.

\section{Live Audio Suggestions}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/Suggestion_live.png}
    \caption{Live Audio Suggestions}
    \label{fig:live_Suggestions}
\end{figure}


\section{File Upload Prediction}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/File_upload_final.png}
    \caption{File Upload Prediction}
    \label{fig:file_upload_prediction}
\end{figure}

This screen demonstrates emotion prediction for uploaded audio files. Users can select pre-recorded audio, and the system predicts the associated emotion accurately.

\section{File Upload Waveforms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/file_uplod_wave_form.png}
    \caption{File Upload Waveforms}
    \label{fig:file_upload_wave_forms}
\end{figure}

Waveforms of uploaded files show audio amplitude variations, providing a visual representation of emotional expression in the recordings.

\section{File Upload Spectrograms}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/fileupload_spectorgram.png}
    \caption{File Upload Spectrograms}
    \label{fig:file_upload_spectrogram}
\end{figure}

Spectrograms for uploaded files visualize frequency distribution over time, helping analyze the characteristics of audio signals and their emotional content.

\section{Emotion Distribution for Uploaded Files}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/emotion_distributio_of_upload.png}
    \caption{Emotion Distribution for Uploaded Files}
    \label{fig:emotion_distribution_upload}
\end{figure}

This chart represents the distribution of predicted emotions from uploaded audio. It validates the consistency and accuracy of predictions compared to live input results.


\section{File Upload Suggestions}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/suggestion_file_upload.png}
    \caption{File Upload Suggestions}
    \label{fig:file_upload}
\end{figure}



\section{History Page}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{img/Graphics/History_final.png}
    \caption{History Page}
    \label{fig:history}
\end{figure}

The history page records previous user interactions, including analyzed audio files and their predicted emotions. This feature provides traceability and insights into user behavior over time.




\clearpage







\section{Training and Validation Loss}
The training loss curve of the model is the quantity of loss being generated by the model in the course of learning. Early in training, the value of loss is high due to the randomly initializing the model parameters and the network has not yet developed any meaningful patterns based on the voice signals. The first epochs show a decrease in loss, which is noticeable, meaning that the model rapidly begins to learn significant acoustic features like the pitch, energy, and spectral aspects regarding various emotions.
The loss reduces progressively as the training is continued. This consistent negative trend indicates that the convolutional neural network is progressively getting its predictions correct by improving its inner weights. As the later epochs approach, the value of the training loss has a low value, meaning that the model has converged and additional learning processes do not materially lower the error. It is seen that this behavior is an indicator that the optimization process is effective and stable.
Validation loss curve has a similar pattern with the training loss. Despite the fact that the validation loss is a little higher than the training loss, both curves are decreasing in a controlled and smooth way. That the training and validation loss differ by a small gap during the training process means that the model was not overfitting the data during training. Sudden spikes or increases in the loss of validation are non-existent, which proves that the model does not decrease or increase its performance when tested on unknown voice samples.

This observation is also supported by the validation accuracy curve. The validation accuracy is lower at the beginning of the training process because at the moment, the model has not learned enough discriminative features. With continued training, the accuracy of validation increases with every epoch. This is an enhancement of the growing capacity of the model to accurately categorize the various emotional states based on voice signals.
In the latter epochs, the validation accuracy becomes stable at a large value and is similar to the curve of the training accuracy. This highly accurate training and validation means that the generalization is well-achieved. The model is trained to learn the meaningful patterns and not memorize the training data that is imperative when deployed in the real world.
All in all, a combination of diminishing training and validation loss and growing and constant validation accuracy testify the fact that the suggested voice emotion recognition model is properly trained. The learning process is not biased, the danger of overfitting is weak, and the model performs well on unseen test samples. These findings indicate the usefulness of the selected CNN model and feature selection method in voice emotion recognition.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/Graphics/Accuracyonlycurve.png}
    \caption{Training and Validation Accuracy Curve}
    \label{fig:enter-label}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/Graphics/lossonlycurve.png}
    \caption{Training and Validation Loss Curve}
    \label{fig:enter-label}
\end{figure}



\section{Confusion Matrix}
The voice emotion recognition model is analyzed by the use of the confusion matrix to find the classification performance on the test data. It makes a comparison of the predicted labels generated by the model and the actual label of emotions. Right predictions lie on the diagonal of the matrix whereas wrong predictions lie on the outside of the diagonal.
The model is highly effective with respect to the recognition of angry emotions with many samples being accurately identified. Angry samples are confused with happy, neutral or surprise emotions only in small percentages. It means that the acoustic characteristics of the anger like elevated intensity and sudden alterations in pitch are well represented by the model.
Disgust emotion is also put in the right category and most of the samples identified. There are few cases of misclassification with happy and neutral emotions, which indicates that there is some overlap between vocal characteristics. Nevertheless, the total disgust recognition is high and acceptable.
One of the best performances in the confusion matrix is depicted through fear emotion. Overwhelmingly most of the fear samples are accurately characterized with a few mistakes. Some slight confusion is noticed in the case of happy and sad emotions but the misclassification rate is low, which points out that the voice patterns associated with fear are highly acquired by the model.
Happy emotion has proven to be a trustworthy emotion with majority of the samples being correctly predicted. Fear and disgust are somewhat misclassified, as is natural because of the similarity of expressive pitch and patterns of energy. Nonetheless, the model has a constant performance in the distinction of happy voice.
Among all categories of emotions, neutral emotion will be the most accurate classification. The majority of samples that are neutral are correctly recognized with only few samples mixed with sad or happy emotion. This finding proves the fact that the model makes it quite clear that neutral voice features can be distinguished.

The classification of sad emotion is good, but a little confusion with the neutral emotion is noticed. This is one of the pitfalls of voice emotion recognition because sad and neutral emotions can be of low energy and low voice tempo. However, most sad samples are rightly predicted.
The emotion of surprise is also good in classification. Majority of the surprise samples are rightly classified with few being incorrectly classified as disgust or happy. The small error in classification is a sign that the model is effective in learning how to react to sudden and high-energy voice, which occurs when a person is surprised.
On the whole, the confusion matrix demonstrates high levels of diagonal dominance, which proves the quality of the suggested model in all classes of emotions. The number of misclassifications is also not as high and they mostly happen between acoustically similar emotions. These findings show the validity and usefulness of the convolutional neural network in voice emotion recognition on unseen test samples.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/Graphics/final_confusionmatrix.png}
    \caption{Confusion Matrix}
    \label{fig:enter-label}
\end{figure}
\clearpage


\section{Best Case Scenario: Voice Emotion Analyzer}
\vspace{0.5cm}
\begin{table}[h!]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
\textbf{Pre-condition} & User has a clear audio file (WAV/MP3) or microphone ready; system is fully operational; all models and weights loaded correctly. \\ \hline
\textbf{Steps} & 
\begin{enumerate}
    \item User selects either Upload Audio or Record via Microphone.
    \item System records or loads the audio successfully.
    \item Audio pre-processing is applied (noise reduction, normalization, MFCC extraction).
    \item Pre-processed audio is passed to the trained emotion recognition model.
    \item Model predicts the emotion with high confidence.
    \item System displays the recognized emotion and shows confidence chart.
    \item User can play back the uploaded or recorded audio using the play button.
\end{enumerate} \\ \hline
\textbf{Expected Result (Success)} & 
\begin{itemize}
    \item Audio Loaded/Recorded: Audio is ready for processing.
    \item Pre-processing Successful: Clean and normalized audio features extracted.
    \item Emotion Predicted: Correct emotion identified with high confidence.
    \item Confidence Chart: Bar chart shows strong prediction for the correct emotion.
    \item Audio Playback: Play button works; audio can be replayed.
    \item User Feedback: Recognized emotion displayed clearly with optional visual cues (color, emoji, etc.).
\end{itemize} \\ \hline
\end{tabular}
\label{tab:best-case-vea}
\caption{Best Case Scenario for Voice Emotion Analyzer}
\end{table}
\clearpage

\section{Worst Case Scenario: Voice Emotion Analyzer}
\vspace{0.5cm}
\begin{table}[h!]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
\textbf{Pre-condition} & Background noise, distorted audio, unsupported audio format, microphone error, or system/mode weights not loaded properly. \\ \hline
\textbf{Steps} & 
\begin{enumerate}
    \item User uploads corrupted/unsupported audio or tries to record via faulty microphone.
    \item System attempts pre-processing but fails to extract clear features.
    \item Emotion recognition model predicts wrong emotion or low-confidence predictions.
    \item Confidence chart shows multiple low-confidence outputs.
    \item Playback may fail or audio may be distorted.
\end{enumerate} \\ \hline
\textbf{Expected Result (Failure)} & 
\begin{itemize}
    \item Audio Load/Record Error: Warning displayed if audio cannot be read or microphone fails.
    \item Pre-processing Failure: Noise or distortion affects feature extraction.
    \item Wrong Prediction: Incorrect emotion displayed or multiple emotions with low confidence.
    \item Confidence Chart: Shows low or ambiguous confidence scores.
    \item Playback Issue: Audio may not play or play incorrectly.
    \item User Prompt: System suggests retrying with clearer audio or checking microphone.
\end{itemize} \\ \hline
\end{tabular}
\label{tab:worst-case-vea}
\caption{Worst Case Scenario for Voice Emotion Analyzer}
\end{table}
\clearpage
